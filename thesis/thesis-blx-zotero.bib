
@misc{isensee_nnu-net_2018,
	title = {{nnU}-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation},
	url = {http://arxiv.org/abs/1809.10486},
	doi = {10.48550/arXiv.1809.10486},
	shorttitle = {{nnU}-Net},
	abstract = {The U-Net was presented in 2015. With its straight-forward and successful architecture it quickly evolved to a commonly used benchmark in medical image segmentation. The adaptation of the U-Net to novel problems, however, comprises several degrees of freedom regarding the exact architecture, preprocessing, training and inference. These choices are not independent of each other and substantially impact the overall performance. The present paper introduces the {nnU}-Net ('no-new-Net'), which refers to a robust and self-adapting framework on the basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking away superfluous bells and whistles of many proposed network designs and instead focus on the remaining aspects that make out the performance and generalizability of a method. We evaluate the {nnU}-Net in the context of the Medical Segmentation Decathlon challenge, which measures segmentation performance in ten disciplines comprising distinct entities, image modalities, image geometries and dataset sizes, with no manual adjustments between datasets allowed. At the time of manuscript submission, {nnU}-Net achieves the highest mean dice scores across all classes and seven phase 1 tasks (except class 1 in {BrainTumour}) in the online leaderboard of the challenge.},
	number = {{arXiv}:1809.10486},
	publisher = {{arXiv}},
	author = {Isensee, Fabian and Petersen, Jens and Klein, Andre and Zimmerer, David and Jaeger, Paul F. and Kohl, Simon and Wasserthal, Jakob and Koehler, Gregor and Norajitra, Tobias and Wirkert, Sebastian and Maier-Hein, Klaus H.},
	urldate = {2023-08-09},
	date = {2018-09-27},
	eprinttype = {arxiv},
	eprint = {1809.10486 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/janiswehen/Zotero/storage/6S7GBYSN/Isensee et al. - 2018 - nnU-Net Self-adapting Framework for U-Net-Based M.pdf:application/pdf;arXiv.org Snapshot:/Users/janiswehen/Zotero/storage/QYNVPSUJ/1809.html:text/html},
}

@misc{simpson_large_2019,
	title = {A large annotated medical image dataset for the development and evaluation of segmentation algorithms},
	url = {http://arxiv.org/abs/1902.09063},
	abstract = {Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts. We sought to create a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. Such a resource would allow: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. Through a multi-institutional effort, we generated a large, curated dataset representative of several highly variable segmentation tasks that was used in a crowd-sourced challenge - the Medical Segmentation Decathlon held during the 2018 Medical Image Computing and Computer Aided Interventions Conference in Granada, Spain. Here, we describe these ten labeled image datasets so that these data may be effectively reused by the research community.},
	number = {{arXiv}:1902.09063},
	publisher = {{arXiv}},
	author = {Simpson, Amber L. and Antonelli, Michela and Bakas, Spyridon and Bilello, Michel and Farahani, Keyvan and van Ginneken, Bram and Kopp-Schneider, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc and Golia-Pernicka, Jennifer and Heckers, Stephan H. and Jarnagin, William R. and {McHugo}, Maureen K. and Napel, Sandy and Vorontsov, Eugene and Maier-Hein, Lena and Cardoso, M. Jorge},
	urldate = {2023-08-09},
	date = {2019-02-24},
	eprinttype = {arxiv},
	eprint = {1902.09063 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/janiswehen/Zotero/storage/WX5AH5GP/Simpson et al. - 2019 - A large annotated medical image dataset for the de.pdf:application/pdf;arXiv.org Snapshot:/Users/janiswehen/Zotero/storage/SYXY9J69/1902.html:text/html},
}

@misc{ronneberger_u-net_2015,
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	shorttitle = {U-Net},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	number = {{arXiv}:1505.04597},
	publisher = {{arXiv}},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	urldate = {2023-08-15},
	date = {2015-05-18},
	eprinttype = {arxiv},
	eprint = {1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/janiswehen/Zotero/storage/RKK2XTKV/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/Users/janiswehen/Zotero/storage/ZNEKD5ML/1505.html:text/html},
}

@misc{oshea_introduction_2015,
	title = {An Introduction to Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1511.08458},
	doi = {10.48550/arXiv.1511.08458},
	abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network ({ANN}). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of {ANN} architecture is that of the Convolutional Neural Network ({CNN}). {CNNs} are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with {ANNs}. This document provides a brief introduction to {CNNs}, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of {ANNs} and machine learning.},
	number = {{arXiv}:1511.08458},
	publisher = {{arXiv}},
	author = {O'Shea, Keiron and Nash, Ryan},
	urldate = {2023-08-18},
	date = {2015-12-02},
	eprinttype = {arxiv},
	eprint = {1511.08458 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/janiswehen/Zotero/storage/32NAVFER/O'Shea und Nash - 2015 - An Introduction to Convolutional Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/janiswehen/Zotero/storage/WZLR8DQT/1511.html:text/html},
}

@misc{ioffe_batch_2015,
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	url = {http://arxiv.org/abs/1502.03167},
	doi = {10.48550/arXiv.1502.03167},
	shorttitle = {Batch Normalization},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on {ImageNet} classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	number = {{arXiv}:1502.03167},
	publisher = {{arXiv}},
	author = {Ioffe, Sergey and Szegedy, Christian},
	urldate = {2023-08-19},
	date = {2015-03-02},
	eprinttype = {arxiv},
	eprint = {1502.03167 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/janiswehen/Zotero/storage/9CGFM3X8/Ioffe und Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/janiswehen/Zotero/storage/NP4ENB5M/1502.html:text/html},
}

@misc{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	number = {{arXiv}:1412.6980},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2023-08-19},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/janiswehen/Zotero/storage/M6VQVRXT/Kingma und Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/janiswehen/Zotero/storage/66MU6NX5/1412.html:text/html},
}

@misc{wang_super-resolution_2023,
	title = {Super-Resolution Based Patch-Free 3D Image Segmentation with High-Frequency Guidance},
	url = {http://arxiv.org/abs/2210.14645},
	abstract = {High resolution ({HR}) 3D images are widely used nowadays, such as medical images like Magnetic Resonance Imaging ({MRI}) and Computed Tomography ({CT}). However, segmentation of these 3D images remains a challenge due to their high spatial resolution and dimensionality in contrast to currently limited {GPU} memory. Therefore, most existing 3D image segmentation methods use patch-based models, which have low inference efficiency and ignore global contextual information. To address these problems, we propose a super-resolution ({SR}) based patch-free 3D image segmentation framework that can realize {HR} segmentation from a global-wise low-resolution ({LR}) input. The framework contains two sub-tasks, of which semantic segmentation is the main task and super resolution is an auxiliary task aiding in rebuilding the high frequency information from the {LR} input. To furthermore balance the information loss with the {LR} input, we propose a High-Frequency Guidance Module ({HGM}), and design an efficient selective cropping algorithm to crop an {HR} patch from the original image as restoration guidance for it. In addition, we also propose a Task-Fusion Module ({TFM}) to exploit the inter connections between segmentation and {SR} task, realizing joint optimization of the two tasks. When predicting, only the main segmentation task is needed, while other modules can be removed for acceleration. The experimental results on two different datasets show that our framework has a four times higher inference speed compared to traditional patch-based methods, while its performance also surpasses other patch-based and patch-free models.},
	number = {{arXiv}:2210.14645},
	publisher = {{arXiv}},
	author = {Wang, Hongyi and Lin, Lanfen and Hu, Hongjie and Chen, Qingqing and Li, Yinhao and Iwamoto, Yutaro and Han, Xian-Hua and Chen, Yen-Wei and Tong, Ruofeng},
	urldate = {2023-08-22},
	date = {2023-07-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2210.14645 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Wang et al. - 2023 - Super-Resolution Based Patch-Free 3D Image Segment.pdf:/Users/janiswehen/Zotero/storage/LNYLV3GB/Wang et al. - 2023 - Super-Resolution Based Patch-Free 3D Image Segment.pdf:application/pdf},
}

@article{zhou_review_2021,
	title = {A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with progress highlights, and future promises},
	volume = {109},
	issn = {0018-9219, 1558-2256},
	url = {http://arxiv.org/abs/2008.09104},
	doi = {10.1109/JPROC.2021.3054390},
	shorttitle = {A review of deep learning in medical imaging},
	abstract = {Since its renaissance, deep learning has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artiﬁcial intelligence ({AI}) era. It is known that the success of {AI} is mostly attributed to the availability of big data with annotations for a single task and the advances in high performance computing. However, medical imaging presents unique challenges that confront deep learning approaches. In this survey paper, we ﬁrst present traits of medical imaging, highlight both clinical needs and technical challenges in medical imaging, and describe how emerging trends in deep learning are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantiﬁcation, etc. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions.},
	pages = {820--838},
	number = {5},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Zhou, S. Kevin and Greenspan, Hayit and Davatzikos, Christos and Duncan, James S. and van Ginneken, Bram and Madabhushi, Anant and Prince, Jerry L. and Rueckert, Daniel and Summers, Ronald M.},
	urldate = {2023-08-22},
	date = {2021-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.09104 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Zhou et al. - 2021 - A review of deep learning in medical imaging Imag.pdf:/Users/janiswehen/Zotero/storage/9YCGFPZT/Zhou et al. - 2021 - A review of deep learning in medical imaging Imag.pdf:application/pdf},
}

@article{antonelli_medical_2022,
	title = {The Medical Segmentation Decathlon},
	volume = {13},
	rights = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-30695-9},
	doi = {10.1038/s41467-022-30695-9},
	abstract = {International challenges have become the de facto standard for comparative assessment of image analysis algorithms. Although segmentation is the most widely investigated medical image processing task, the various challenges have been organized to focus only on specific clinical tasks. We organized the Medical Segmentation Decathlon ({MSD})—a biomedical image analysis challenge, in which algorithms compete in a multitude of both tasks and modalities to investigate the hypothesis that a method capable of performing well on multiple tasks will generalize well to a previously unseen task and potentially outperform a custom-designed solution. {MSD} results confirmed this hypothesis, moreover, {MSD} winner continued generalizing well to a wide range of other clinical problems for the next two years. Three main conclusions can be drawn from this study: (1) state-of-the-art image segmentation algorithms generalize well when retrained on unseen tasks; (2) consistent algorithmic performance across multiple tasks is a strong surrogate of algorithmic generalizability; (3) the training of accurate {AI} segmentation models is now commoditized to scientists that are not versed in {AI} model training.},
	pages = {4128},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Antonelli, Michela and Reinke, Annika and Bakas, Spyridon and Farahani, Keyvan and Kopp-Schneider, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and van Ginneken, Bram and Bilello, Michel and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc J. and Heckers, Stephan H. and Huisman, Henkjan and Jarnagin, William R. and {McHugo}, Maureen K. and Napel, Sandy and Pernicka, Jennifer S. Golia and Rhode, Kawal and Tobon-Gomez, Catalina and Vorontsov, Eugene and Meakin, James A. and Ourselin, Sebastien and Wiesenfarth, Manuel and Arbeláez, Pablo and Bae, Byeonguk and Chen, Sihong and Daza, Laura and Feng, Jianjiang and He, Baochun and Isensee, Fabian and Ji, Yuanfeng and Jia, Fucang and Kim, Ildoo and Maier-Hein, Klaus and Merhof, Dorit and Pai, Akshay and Park, Beomhee and Perslev, Mathias and Rezaiifar, Ramin and Rippel, Oliver and Sarasua, Ignacio and Shen, Wei and Son, Jaemin and Wachinger, Christian and Wang, Liansheng and Wang, Yan and Xia, Yingda and Xu, Daguang and Xu, Zhanwei and Zheng, Yefeng and Simpson, Amber L. and Maier-Hein, Lena and Cardoso, M. Jorge},
	urldate = {2023-08-22},
	date = {2022-07-15},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Three-dimensional imaging, Translational research},
	file = {Full Text PDF:/Users/janiswehen/Zotero/storage/D7MDZRG6/Antonelli et al. - 2022 - The Medical Segmentation Decathlon.pdf:application/pdf},
}

@article{lee_automatic_2020,
	title = {Automatic segmentation of brain {MRI} using a novel patch-wise U-net deep architecture},
	volume = {15},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7398543/},
	doi = {10.1371/journal.pone.0236493},
	abstract = {Accurate segmentation of brain magnetic resonance imaging ({MRI}) is an essential step in quantifying the changes in brain structure. Deep learning in recent years has been extensively used for brain image segmentation with highly promising performance. In particular, the U-net architecture has been widely used for segmentation in various biomedical related fields. In this paper, we propose a patch-wise U-net architecture for the automatic segmentation of brain structures in structural {MRI}. In the proposed brain segmentation method, the non-overlapping patch-wise U-net is used to overcome the drawbacks of conventional U-net with more retention of local information. In our proposed method, the slices from an {MRI} scan are divided into non-overlapping patches that are fed into the U-net model along with their corresponding patches of ground truth so as to train the network. The experimental results show that the proposed patch-wise U-net model achieves a Dice similarity coefficient ({DSC}) score of 0.93 in average and outperforms the conventional U-net and the {SegNet}-based methods by 3\% and 10\%, respectively, for on Open Access Series of Imaging Studies ({OASIS}) and Internet Brain Segmentation Repository ({IBSR}) dataset.},
	pages = {e0236493},
	number = {8},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} One},
	author = {Lee, Bumshik and Yamanakkanavar, Nagaraj and Choi, Jae Young},
	urldate = {2023-08-23},
	date = {2020-08-03},
	pmid = {32745102},
	pmcid = {PMC7398543},
	file = {PubMed Central Full Text PDF:/Users/janiswehen/Zotero/storage/KUFA7GVG/Lee et al. - 2020 - Automatic segmentation of brain MRI using a novel .pdf:application/pdf},
}

@misc{chong_resource_2023,
	title = {Resource Efficient Neural Networks Using Hessian Based Pruning},
	url = {http://arxiv.org/abs/2306.07030},
	abstract = {Neural network pruning is a practical way for reducing the size of trained models and the number of floating point operations ({FLOPs}). One way of pruning is to use the relative Hessian trace to calculate sensitivity of each channel, as compared to the more common magnitude pruning approach. However, the stochastic approach used to estimate the Hessian trace needs to iterate over many times before it can converge. This can be time-consuming when used for larger models with many millions of parameters. To address this problem, we modify the existing approach by estimating the Hessian trace using {FP}16 precision instead of {FP}32. We test the modified approach ({EHAP}) on {ResNet}-32/{ResNet}-56/{WideResNet}-28-8 trained on {CIFAR}10/{CIFAR}100 image classification tasks and achieve faster computation of the Hessian trace. Specifically, our modified approach can achieve speed ups ranging from 17\% to as much as 44\% during our experiments on different combinations of model architectures and {GPU} devices. Our modified approach also takes up ∼40\% less {GPU} memory when pruning {ResNet}-32 and {ResNet}-56 models, which allows for a larger Hessian batch size to be used for estimating the Hessian trace. Meanwhile, we also present the results of pruning using both {FP}16 and {FP}32 Hessian trace calculation and show that there is no noticeable accuracy differences between the two. Overall, it is a simple and effective way to compute the relative Hessian trace faster without sacrificing on pruned model performance. We also present a full pipeline using {EHAP} and quantization aware training ({QAT}), using {INT}8 {QAT} to compress the network further after pruning. In particular, we use symmetric quantization for the weights and asymmetric quantization for the activations. The framework has been open-sourced and the code is available at https://github.com/{JackkChong}/Resource-{EfficientNeural}-Networks-Using-Hessian-Based-Pruning.},
	number = {{arXiv}:2306.07030},
	publisher = {{arXiv}},
	author = {Chong, Jack and Gupta, Manas and Chen, Lihui},
	urldate = {2023-08-25},
	date = {2023-06-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2306.07030 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {Chong et al. - 2023 - Resource Efficient Neural Networks Using Hessian B.pdf:/Users/janiswehen/Zotero/storage/6PDQ2S9F/Chong et al. - 2023 - Resource Efficient Neural Networks Using Hessian B.pdf:application/pdf},
}

@misc{gholami_survey_2021,
	title = {A Survey of Quantization Methods for Efficient Neural Network Inference},
	url = {http://arxiv.org/abs/2103.13630},
	abstract = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efﬁcient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a ﬁxed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from ﬂoating-point representations to low-precision ﬁxed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efﬁcient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.},
	number = {{arXiv}:2103.13630},
	publisher = {{arXiv}},
	author = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
	urldate = {2023-08-25},
	date = {2021-06-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2103.13630 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Gholami et al. - 2021 - A Survey of Quantization Methods for Efficient Neu.pdf:/Users/janiswehen/Zotero/storage/J4KG3W2W/Gholami et al. - 2021 - A Survey of Quantization Methods for Efficient Neu.pdf:application/pdf},
}

@misc{brugger_partially_2019,
	title = {A Partially Reversible U-Net for Memory-Efficient Volumetric Image Segmentation},
	url = {http://arxiv.org/abs/1906.06148},
	abstract = {One of the key drawbacks of 3D convolutional neural networks for segmentation is their memory footprint, which necessitates compromises in the network architecture in order to ﬁt into a given memory budget. Motivated by the {RevNet} for image classiﬁcation, we propose a partially reversible U-Net architecture that reduces memory consumption substantially. The reversible architecture allows us to exactly recover each layer’s outputs from the subsequent layer’s ones, eliminating the need to store activations for backpropagation. This alleviates the biggest memory bottleneck and enables very deep (theoretically inﬁnitely deep) 3D architectures. On the {BraTS} challenge dataset, we demonstrate substantial memory savings. We further show that the freed memory can be used for processing the whole ﬁeld-of-view ({FOV}) instead of patches. Increasing network depth led to higher segmentation accuracy while growing the memory footprint only by a very small fraction, thanks to the partially reversible architecture.},
	number = {{arXiv}:1906.06148},
	publisher = {{arXiv}},
	author = {Brügger, Robin and Baumgartner, Christian F. and Konukoglu, Ender},
	urldate = {2023-08-25},
	date = {2019-06-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.06148 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Brügger et al. - 2019 - A Partially Reversible U-Net for Memory-Efficient .pdf:/Users/janiswehen/Zotero/storage/LMG4H3AW/Brügger et al. - 2019 - A Partially Reversible U-Net for Memory-Efficient .pdf:application/pdf},
}
